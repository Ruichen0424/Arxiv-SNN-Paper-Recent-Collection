# Arxiv SNN Paper Weekly


 ### **Last 5 working days (Updated on 2025-01-22)** 


- Learnable Sparsification of Die-to-Die Communication via Spike-Based Encoding [[arxiv](https://arxiv.org/abs/2501.08645)]

- Self-Attentive Spatio-Temporal Calibration for Precise Intermediate Layer Matching in ANN-to-SNN Distillation [[arxiv](https://arxiv.org/abs/2501.08049)] [[paper with code](https://paperswithcode.com/paper/self-attentive-spatio-temporal-calibration)]

- Spiking Neural Network Accelerator Architecture for Differential-Time Representation using Learned Encoding [[arxiv](https://arxiv.org/abs/2501.07952)] [[paper with code](https://paperswithcode.com/paper/spiking-neural-network-accelerator)]

- An Efficient Sparse Hardware Accelerator for Spike-Driven Transformer [[arxiv](https://arxiv.org/abs/2501.07825)]

- Efficient Event-based Delay Learning in Spiking Neural Networks [[arxiv](https://arxiv.org/abs/2501.07331)] [[paper with code](https://paperswithcode.com/paper/efficient-event-based-delay-learning-in)] [[code](https://github.com/mbalazs98/deventprop)]

- SPAM: Spike-Aware Adam with Momentum Reset for Stable LLM Training [[arxiv](https://arxiv.org/abs/2501.06842)] [[paper with code](https://paperswithcode.com/paper/spam-spike-aware-adam-with-momentum-reset-for)] [[code](https://github.com/tianjinyellow/spam-optimizer)]

- Temporal-Aware Spiking Transformer Hashing Based on 3D-DWT [[arxiv](https://arxiv.org/abs/2501.06786)] [[paper with code](https://paperswithcode.com/paper/temporal-aware-spiking-transformer-hashing)]

